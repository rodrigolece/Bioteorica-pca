{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis\n",
    "\n",
    "\n",
    "#### De Wikipedia:\n",
    "\n",
    "Principal component analysis (PCA) is a statistical procedure that uses an **orthogonal transformation** to convert a set of observations of **possibly correlated variables** into a set of values of **linearly uncorrelated variables** called principal components. The number of principal components is less than or equal to the number of original variables. This transformation is defined in such a way that the first principal component has the largest possible variance (that is, accounts for as much of the variability in the data as possible), and each succeeding component in turn has the highest variance possible under the constraint that it is orthogonal to the preceding components. The resulting vectors are an uncorrelated orthogonal basis set. The principal components are orthogonal because they are the eigenvectors of the covariance matrix, which is symmetric. PCA is sensitive to the relative scaling of the original variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vamos a generar datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gaussiana2D(prom, sigma, cov, n):\n",
    "    np.random.seed(142)\n",
    "    return np.random.multivariate_normal(np.array([prom, prom]), np.array([[sigma, cov], [cov, sigma]]), n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datosNoCorrel = gaussiana2D(prom=50, sigma=1, cov=0, n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "plt.grid()\n",
    "plt.xlabel(r'$x_1$') , plt.ylabel(r'$x_2$')\n",
    "plt.scatter(datosNoCorrel[:,0], datosNoCorrel[:,1], s=14**2, alpha=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datosCorrel = gaussiana2D(prom=50, sigma=1, cov=0.9, n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "plt.grid()\n",
    "plt.xlabel(r'$x_1$') , plt.ylabel(r'$x_2$')\n",
    "plt.scatter(datosCorrel[:,0], datosCorrel[:,1], s=14**2, alpha=0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Centramos los datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_ptos = len(datosCorrel)\n",
    "prom = datosCorrel.sum(0)/num_ptos # sum(0) suma sobre las columnas\n",
    "\n",
    "print prom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datosCorrelPromCero = datosCorrel - prom\n",
    "\n",
    "print datosCorrel[0,:]\n",
    "print datosCorrelPromCero[0,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculamos la matriz de covarianza: \n",
    "\n",
    "Si  $\\scriptsize \\mathbf{X} \\in \\mathbb{R}^{n \\times d}$ es nuestra matriz de datos centrados en el origen, la matriz de covarianza está dada por:\n",
    "\n",
    "$$ \\mathbf{C}_{\\mathbf X} = \\frac{1}{n} \\mathbf{X}^\\top \\mathbf{X} \\,.$$\n",
    "\n",
    "Para que lo que estamos haciendo pueda ser paralelizable (en un momento explicaré por qué queremos ésto), en lugar de calcular directamente el producto de $\\scriptsize \\mathbf{X}^\\top \\mathbf{X}$ vamos a calcular el producto de estas dos matrices como la suma de productos exteriores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "covCorrel = sum( map(lambda pto: np.outer(pto, pto), datosCorrelPromCero) ) / float(num_ptos)\n",
    "\n",
    "print covCorrel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Ejercicio: \n",
    "\n",
    "Escribir una función para calcular la covarianza de un conjunto de datos. Recibe como argumento un arreglo de puntos y regresa la matriz de covarianza. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def covarianza(datos):\n",
    "    ## Llenar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Descomposición en eigenvalores:\n",
    "\n",
    "Podemos usar la matriz de covarianza para encontrar las direcciones de máxima variación. Ésto se hace descomponiendo a la matriz para encontrar sus eigenvalores y eigenvectores. Una matriz en $\\scriptsize \\mathbb{R}^{d \\times d}$, tiene $d$ eigenvalores y eigenvectore (no necesariamente distintos). \n",
    "\n",
    "Recordamos que $\\scriptsize \\lambda$ es un eigenvalor de $\\scriptsize \\mathbf{X} \\in \\mathbb{R}^{d \\times d}$ si existe $\\scriptsize \\mathbf{v}_{\\lambda}$ tal que:\n",
    "\n",
    "$$ \\mathbf{X} \\, \\mathbf{v}_{\\lambda} = \\lambda \\, \\mathbf{v}_{\\lambda}$$\n",
    "\n",
    "Para cada eigenvalor $\\scriptsize \\lambda$ existen infinidad de vectoress que satisfacen la ecuación de arriba, por lo que por conveción tomamos $\\scriptsize \\mathbf{v}_{\\lambda}$ unitario. \n",
    "\n",
    "Nos interesa quedarnos con las $k$ direcciones que concentran la mayor variación, esto es, queremos encontrar los $k$ eigenvectores asociados a los $k$ eigenvalores más grandes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.13820481  1.94345403] \n",
      "\n",
      "[[-0.72461254  0.68915649]\n",
      " [ 0.68915649  0.72461254]]\n"
     ]
    }
   ],
   "source": [
    "from numpy.linalg import eigh\n",
    "\n",
    "eigVals, eigVecs = eigh(covCorrel)\n",
    "\n",
    "print eigVals, '\\n'\n",
    "print eigVecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordenamos a los eigenvalores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inds = np.argsort(eigVals)\n",
    "top = eigVecs[:,inds[-1]]\n",
    "\n",
    "print top"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculamos  las \"calificaciones\" (scores) de los datos originales:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[70.516828061106537, 69.306223556234471, 71.135881678263345]\n"
     ]
    }
   ],
   "source": [
    "scoresCorrel = map(lambda pto: pto.dot(top), datosCorrel)\n",
    "\n",
    "print scoresCorrel[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Ejercicio:\n",
    "\n",
    "Escribir la función que hace el análisis de **PCA** entero. Recibe como argumento los datos y `k` el número de componentes principales que uno va a conservar. Regresa las `k` componentes principales (los eigenvectores), las calificaciones de los datos originales, y los `k` eigenvalores principales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pca(datos, k=2):\n",
    "    cov = covarianza(datos)\n",
    "    \n",
    "    eigVals, eigVecs = eigh(cov)\n",
    "    # Ordenar y sólo tomar k eigenVals y eigenVecs\n",
    "    \n",
    "    # Calcular calificaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Qué porcentaje de la varianza se explica con `k` componentes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def varianzaExplicada(datos, k=1):\n",
    "    components, scores, eigenvalues = pca(data,k)\n",
    "    \n",
    "    cum = eigenvalues.cumsum()\n",
    "    \n",
    "    return cum[k-1]/cum[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "noCorrel1 = varianzaExplicada(datosNoCorrel, 1)\n",
    "noCorrel2 = varianzaExplicada(datosNoCorrel, 2)\n",
    "correl1 = varianzaExplicada(datosCorrel, 1)\n",
    "correl2 = varianzaExplicada(datosCorrel, 2)\n",
    "\n",
    "print 100*noCorrel1\n",
    "print 100*noCorrel2, \\n\n",
    "print 100*correl1\n",
    "print 100*correl2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
